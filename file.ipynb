{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .util import *\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_multiline_key_value_string(text, to_json=False):\n",
    "    \"\"\"\n",
    "    Parses a string with key-value pairs separated by newlines into a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        text (str): String with format 'Key: Value\\\\nKey: Value\\\\n...'\n",
    "        to_json (bool): If True, returns a JSON string instead of a dict.\n",
    "        \n",
    "    Returns:\n",
    "        dict or str: Parsed data as a dict or JSON string.\n",
    "    \"\"\"\n",
    "    lines = text.strip().split('\\n')\n",
    "    data = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            data[key.strip()] = value.strip()\n",
    "\n",
    "    return json.dumps(data, indent=2) if to_json else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"database/myDB\")\n",
    "collection = client.get_or_create_collection(name=\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['Book2.xlsx_0', 'daniel.csv_0', 'daniel.csv_1', 'daniel.csv_2'], 'embeddings': None, 'documents': ['', 'Document Number: 1\\nContent: My name is Pythagoras\\nCategory: Beauty', 'Document Number: 2\\nContent: Trust and obey\\nCategory: Finance', 'Document Number: 3\\nContent: For there is no other way\\nCategory: Love'], 'uris': None, 'data': None, 'metadatas': None, 'included': [<IncludeEnum.documents: 'documents'>]}\n"
     ]
    }
   ],
   "source": [
    "results = collection.get(include=['documents'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document Number: 1\\nContent: My name is Pythagoras\\nCategory: Beauty'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Pythagoras'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "parse_multiline_key_value_string(results['documents'][1])[\"Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielstephens/Desktop/TOVA/topic/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_embed(\n",
    "    df: pd.DataFrame,\n",
    "    text_columns: list,\n",
    "    sbert_model: str = \"paraphrase-distilroberta-base-v2\",\n",
    "    tfidf_threshold: float = 3,\n",
    "    batch_size: int = 32\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess texts and compute embeddings for database insertion.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing text columns.\n",
    "        text_columns (List[str]): List of columns to be concatenated and preprocessed.\n",
    "        sbert_model (str): SentenceTransformer model name.\n",
    "        tfidf_threshold (float): Threshold for tf-idf based stopword extension.\n",
    "        batch_size (int): Batch size for embedding calculation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with an additional 'embedding' column.\n",
    "    \"\"\"\n",
    "    print(\"ðŸš© Preprocessing started...\")\n",
    "\n",
    "    # Remove rows with missing text\n",
    "    df = df.dropna(subset=text_columns, how=\"any\")\n",
    "\n",
    "    # Concatenate selected columns\n",
    "    df[\"calculate_on\"] = df.apply(lambda row: ' '.join([str(row[col]) for col in text_columns]), axis=1)\n",
    "\n",
    "    texts = df[\"calculate_on\"].tolist()\n",
    "\n",
    "    # Setup NLP\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    # Create doc objects\n",
    "    docs = list(nlp.pipe(texts))\n",
    "\n",
    "    # TF-IDF to extend stopwords\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(texts)\n",
    "    tfidf_stopwords = set([word for word, idf in zip(vectorizer.get_feature_names_out(), vectorizer.idf_) if idf <= tfidf_threshold])\n",
    "    stop_words = STOP_WORDS.union(tfidf_stopwords)\n",
    "\n",
    "    # Tokenization & Cleaning\n",
    "    processed_texts = []\n",
    "    for doc in docs:\n",
    "        tokens = [token.lemma_.lower() for token in doc if\n",
    "                  re.search('[a-z0-9]+', token.text) and\n",
    "                  len(token.text) > 1 and\n",
    "                  not token.is_digit and\n",
    "                  not token.is_space and\n",
    "                  token.lemma_.lower() not in stop_words]\n",
    "        cleaned_tokens = [''.join([char for char in tok if char.isalpha()]) for tok in tokens]\n",
    "        cleaned_tokens = [tok for tok in cleaned_tokens if tok.strip()]\n",
    "        processed_texts.append(' '.join(cleaned_tokens))\n",
    "\n",
    "    # Embedding\n",
    "    print(\"ðŸš© Calculating embeddings...\")\n",
    "    model = SentenceTransformer(sbert_model)\n",
    "    embeddings = model.encode(processed_texts, batch_size=batch_size, show_progress_bar=True)\n",
    "\n",
    "    # Add columns for processed content and embeddings\n",
    "    df[\"processed_text\"] = processed_texts\n",
    "    df[\"embedding\"] = [embedding.tolist() for embedding in embeddings]  # convert np.ndarray to list for JSON/DB\n",
    "\n",
    "    print(\"âœ… Preprocessing completed!\")\n",
    "    return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_embed(\n",
    "    df: pd.DataFrame,\n",
    "    text_columns: list,\n",
    "    sbert_model: str = \"paraphrase-distilroberta-base-v2\",\n",
    "    tfidf_threshold: float = 3,\n",
    "    batch_size: int = 32,\n",
    "    min_tokens: int = 20  # threshold for short preprocessed text\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess texts and compute embeddings for database insertion.\n",
    "    \n",
    "    If preprocessed text is too short, fallback to the original text.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing text columns.\n",
    "        text_columns (List[str]): List of columns to be concatenated and preprocessed.\n",
    "        sbert_model (str): SentenceTransformer model name.\n",
    "        tfidf_threshold (float): Threshold for tf-idf based stopword extension.\n",
    "        batch_size (int): Batch size for embedding calculation.\n",
    "        min_tokens (int): Minimum number of tokens required to accept preprocessed text.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with an additional 'embedding' column.\n",
    "    \"\"\"\n",
    "    print(\"ðŸš© Preprocessing started...\")\n",
    "\n",
    "    # Remove rows with missing text\n",
    "    df = df.dropna(subset=text_columns, how=\"any\")\n",
    "\n",
    "    # Concatenate selected columns\n",
    "    df[\"calculate_on\"] = df.apply(lambda row: ' '.join([str(row[col]) for col in text_columns]), axis=1)\n",
    "\n",
    "    texts = df[\"calculate_on\"].tolist()\n",
    "\n",
    "    # Setup NLP\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    # Create doc objects\n",
    "    docs = list(nlp.pipe(texts))\n",
    "\n",
    "    # TF-IDF to extend stopwords\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(texts)\n",
    "    tfidf_stopwords = set([word for word, idf in zip(vectorizer.get_feature_names_out(), vectorizer.idf_) if idf <= tfidf_threshold])\n",
    "    stop_words = STOP_WORDS.union(tfidf_stopwords)\n",
    "\n",
    "    # Tokenization & Cleaning with fallback\n",
    "    processed_texts = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        tokens = [token.lemma_.lower() for token in doc if\n",
    "                  re.search('[a-z0-9]+', token.text) and\n",
    "                  len(token.text) > 1 and\n",
    "                  not token.is_digit and\n",
    "                  not token.is_space and\n",
    "                  token.lemma_.lower() not in stop_words]\n",
    "        cleaned_tokens = [''.join([char for char in tok if char.isalpha()]) for tok in tokens]\n",
    "        cleaned_tokens = [tok for tok in cleaned_tokens if tok.strip()]\n",
    "        \n",
    "        if len(cleaned_tokens) < min_tokens:\n",
    "            processed_texts.append(df[\"calculate_on\"].iloc[i])\n",
    "        else:\n",
    "            processed_texts.append(' '.join(cleaned_tokens))\n",
    "\n",
    "    # Embedding\n",
    "    print(\"ðŸš© Calculating embeddings...\")\n",
    "    model = SentenceTransformer(sbert_model)\n",
    "    embeddings = model.encode(processed_texts, batch_size=batch_size, show_progress_bar=True)\n",
    "\n",
    "    # Add columns for processed content and embeddings\n",
    "    df[\"processed_text\"] = processed_texts\n",
    "    df[\"embedding\"] = [embedding.tolist() for embedding in embeddings]\n",
    "\n",
    "    print(\"âœ… Preprocessing completed!\")\n",
    "    return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš© Preprocessing started...\n",
      "ðŸš© Calculating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocess_and_embed(pd.read_csv(\"data/betty.csv\"), [\"Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Number</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>calculate_on</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>My name is Betty</td>\n",
       "      <td>General</td>\n",
       "      <td>My name is Betty</td>\n",
       "      <td>My name is Betty</td>\n",
       "      <td>[0.38925281167030334, 0.9264926910400391, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Found only in the heart cardiac muscle is resp...</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Found only in the heart cardiac muscle is resp...</td>\n",
       "      <td>find pump control hormone signal stimulate sti...</td>\n",
       "      <td>[0.26081937551498413, 0.1862926036119461, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Move for me everything will be alright</td>\n",
       "      <td>Health</td>\n",
       "      <td>Move for me everything will be alright</td>\n",
       "      <td>Move for me everything will be alright</td>\n",
       "      <td>[0.1871119886636734, 0.5584659576416016, 0.219...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document Number                                            Content  \\\n",
       "0                1                                   My name is Betty   \n",
       "1                2  Found only in the heart cardiac muscle is resp...   \n",
       "2                3             Move for me everything will be alright   \n",
       "\n",
       "  Category                                       calculate_on  \\\n",
       "0  General                                   My name is Betty   \n",
       "1  Finance  Found only in the heart cardiac muscle is resp...   \n",
       "2   Health             Move for me everything will be alright   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0                                   My name is Betty   \n",
       "1  find pump control hormone signal stimulate sti...   \n",
       "2             Move for me everything will be alright   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.38925281167030334, 0.9264926910400391, -0.2...  \n",
       "1  [0.26081937551498413, 0.1862926036119461, -0.0...  \n",
       "2  [0.1871119886636734, 0.5584659576416016, 0.219...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed[\"processed_text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
