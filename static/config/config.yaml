################################
#     LOGGING CONFIGURATION    #
################################
logger:
  dir_logger: data/logs
  console_log: True
  file_log: True
  log_level: INFO
  logger_name: tova
  N_log_keep: 5 #maximum number of log files to keep

################################
# TOPIC MODELING CONFIGURATION #
################################
topic_modeling:
  # general configuration for any topic model, plus specific configuration for either traditional or llm-based
  general:
    not_include: ['train_data', 'df', 'embeddings', 'ids_corpus', '_logger', '_embedding_model', '_umap_model', '_hdbscan_model', '_vectorizer_model', '_ctfidf_model', '_representation_model', '_model']
    n_similar_tpcs: 5
    similar_tpcs_thr: 1e-3
    n_top_docs_per_topic: 20
    llm_provider: "ollama"
    llm_model_type: "gemma3:4b"
    llm_server: "http://kumo01.tsc.uc3m.es:11434"
  traditional:
    num_topics: 15
    thetas_thr: 3e-3
    topn: 15
    do_labeller: True
    do_summarizer: True
    labeller_prompt: "src/tova/prompter/prompts/labelling_dft.txt"
    summarizer_prompt: "src/tova/prompter/prompts/summarization_dft.txt"
    llm_provider: "ollama"
    llm_model_type: "gemma3:4b"
    llm_server: "http://kumo01.tsc.uc3m.es:11434"
  llm-based:

  # per topic model configs
  mallet:
    alpha: 5.0
    optimize_interval: 10
    num_threads: 4
    num_iters: 1000
    doc_topic_thr: 0.0
    token_regexp: "[\\p{L}\\p{N}][\\p{L}\\p{N}\\p{P}]*\\p{L}"
    mallet_path: src/train/Mallet-202108/bin/mallet
  tomotopyLDA:
    num_iters: 2000
    alpha: 5.0
    eta: 0.01
    iter_interval: 10
  bertopic:
    no_below: 1
    no_above: 1.0
    stopwords: None
    sbert_model: all-MiniLM-L6-v2
    umap_n_components: 5
    umap_n_neighbors: 15
    umap_min_dist: 0.0
    umap_metric: cosine
    hdbscan_min_cluster_size: 10
    hdbscan_metric: euclidean
    hdbscan_cluster_selection_method: eom
    hbdsan_prediction_data: True
    language: english
    repr_model_diversity: 0.3
    repr_model_topnwords: 15
    word_min_len: 0
  ctm:
    num_epochs: 2
    sbert_model: all-MiniLM-L6-v2
    sbert_context: 384
    batch_size: 32
    contextual_size: 384
    inference_type: "combined"
    n_components: 10
    model_type: "prodLDA"
    hidden_sizes: [100, 100]
    activation: "softplus"
    dropout: 0.2
    learn_priors: true
    lr: 0.002
    momentum: 0.99
    solver: "adam"
    reduce_on_plateau: false
    num_data_loader_workers: 4
    label_size: 0
    loss_weights: null
  topicgpt:
    sample: 10 #0.001
    temperature: 0.0
    top_p: 0.0
    max_tokens_gen1: 300
    max_tokens_gen2: 500
    max_tokens_assign: 300
    refined_again: false
    remove: false
    do_second_level: false
    verbose: true
  opentopicrag:
    run_from_web: True # @Daniel, I'm not entirely sure how to display this. This parameter shouldn't be visible to the user in the web interface and should be set to True by default. However, if it's not provided through the web, I need to request the input from the user via stdin.
    embedding_model: Qwen/Qwen3-Embedding-0.6B
    tp_generation_prompt_path: src/tova/prompter/prompts/otrag_tp_generation_dft.txt
    doc_labelling_prompt_path: src/tova/prompter/prompts/otrag_doc_labelling_dft.txt
    num_docs_in_prompt: 5
    max_feat_tfidf: 1000
    user_preferences: 
    sample: 100
    nr_iterations: 2

################################
#       PREPROCESSING          #
################################
tm_preprocessor:
  spacy:
    spacy_model: en_core_web_sm
    spacy_disable: ["ner", "parser"]
    valid_pos: ["VERB", "NOUN", "ADJ", "PROPN"]
  vectorization:
    max_features: 100000
    min_df: 1
    max_df: 1
    ngram_range: [1, 2]
    binary: false
  embeddings:
    model_name: all-MiniLM-L6-v2
    normalize: true
    batch_size: 32

################################
#          PROMPTER            #
################################
llm:
  parameters:
    temperature: 0
    top_p: 0.1
    frequency_penalty: 0.0
    random_seed: 1234
    seed: 1234
  gpt:
    available_models:
      {
        "gpt-4o-2024-08-06",
        "gpt-4o-mini-2024-07-18",
        "chatgpt-4o-latest",
        "gpt-4-turbo",
        "gpt-4-turbo-2024-04-09",
        "gpt-4",
        "gpt-3.5-turbo",
        "gpt-4o-mini",
        "gpt-4o",
        "gpt-4-32k",
        "gpt-4-0125-preview",
        "gpt-4-1106-preview",
        "gpt-4-vision-preview",
        "gpt-3.5-turbo-0125",
        "gpt-3.5-turbo-instruct",
        "gpt-3.5-turbo-1106",
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-16k-0613",
        "gpt-3.5-turbo-0301",
      }
    path_api_key: .env
  ollama:
    available_models: {
      "llama3.2",
      "llama3.1:8b-instruct-q8_0",
      "qwen:32b",
      "llama3.3:70b",
      "qwen2.5:32b",
      "gemma3:4b"
    }
    host: http://kumo01.tsc.uc3m.es:11434
  llama_cpp:
    host: http://kumo01:11435/v1/chat/completions